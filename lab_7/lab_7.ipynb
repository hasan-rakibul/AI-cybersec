{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1E62EZC7bhFe93yHTHvzo_3uCvzZKC8GO",
      "authorship_tag": "ABX9TyONwH01CSWQBcwFJ1+5UZhq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasan-rakibul/AI-cybersec/blob/main/lab_7/lab_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spam fighting using machine learning"
      ],
      "metadata": {
        "id": "s5PijsUCmpn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reference book:** Chio, C., & Freeman, D. (2018). *Machine learning and security: Protecting systems with data and algorithms* (First edition). Oâ€™Reilly Media. (page 18)\n",
        "\n",
        "**Reference programs:** https://github.com/oreilly-mlsec/book-resources/tree/master/chapter1"
      ],
      "metadata": {
        "id": "R0nQ3bNKmn-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Access dataset"
      ],
      "metadata": {
        "id": "37dMGHl0WSuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the 2007 TREC Public Spam Corpus dataset that to explore spam versus ham (not spam) classification problem. This is a lightly cleaned raw email message corpus containing 75,419 messages collected from an email server over a three-month period in 2007. One-third of the dataset is made up of spam examples, and the rest is ham."
      ],
      "metadata": {
        "id": "ExCepMxvyYMF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTQvuQkPhk4_"
      },
      "source": [
        "### Downloading 2007 TREC Public Spam Corpus\n",
        "1. Read and accept the \"Agreement for use\"\n",
        "   [https://plg.uwaterloo.ca/~gvcormac/treccorpus07/](https://plg.uwaterloo.ca/~gvcormac/treccorpus07/)\n",
        "2. You will find \"255 MB Corpus (trec07p.tgz)\". Right click on that link and \"save link as\" the dataset. You can keep the data on your Google drive folder (e.g., Colab\n",
        "Notebooks/AICS/ folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "eMqxdtn0JJeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, mount Google Drive\n",
        "if not os.path.exists(\"trec07p\"):\n",
        "  print(\"Starting to uncompress the file ...\")\n",
        "  !tar xzf \"/content/drive/MyDrive/Colab Notebooks/AICS/trec07p.tgz\""
      ],
      "metadata": {
        "id": "YfUQcVuavGkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above command can be read as \"extract the contents of the `trec07p.tgz` archive file located in the directory `/content/drive/MyDrive/Colab Notebooks/AICS`.\n",
        "\n",
        "* tar is a command-line utility in Unix-based operating systems used for working with TAR archives.\n",
        "* x is a command-line option used to extract the contents of the archive.\n",
        "* z is a command-line option used to decompress the archive using the gzip utility before extracting its contents.\n",
        "* f is a command-line option used to specify the name of the archive file to work with."
      ],
      "metadata": {
        "id": "dRFkNbAn4pE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: Your computer may not allow you to download and store the Spam database due to security reasons. **As an alternative,** You can follow the following steps, which basically download it from [Kaggle](https://www.kaggle.com/datasets/bayes2003/emails-for-spam-or-ham-classification-trec-2007), unzip it, and finally, uncompress the tar ball."
      ],
      "metadata": {
        "id": "GLmMmSaLRayZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o trec-2007.zip\\\n",
        "  https://www.kaggle.com/api/v1/datasets/download/bayes2003/emails-for-spam-or-ham-classification-trec-2007\n",
        "\n",
        "!unzip trec-2007.zip\n",
        "\n",
        "!tar xzf trec07p.tgz"
      ],
      "metadata": {
        "id": "LWwRMuByMgsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = './trec07p/data/'\n",
        "LABELS_FILE = './trec07p/full/index'"
      ],
      "metadata": {
        "id": "F8GSD2MyU1pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Email processing"
      ],
      "metadata": {
        "id": "CWF_pPccyzit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first define a set of functions to help with loading and preprocessing the data and labels, as demonstrated in the following code."
      ],
      "metadata": {
        "id": "KBi1GgfnsjYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import email"
      ],
      "metadata": {
        "id": "MvtkPpuyqQy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract subject and body text from a signle email file\n",
        "def extract_email_text(path):\n",
        "  # Load a single email file from an input file\n",
        "  with open(path, errors='ignore') as f:\n",
        "    msg = email.message_from_file(f)\n",
        "  if not msg:\n",
        "    return \"\"\n",
        "\n",
        "  # Read the email subject\n",
        "  subject = msg['Subject']\n",
        "  if not subject:\n",
        "    subject = ''\n",
        "\n",
        "  # Read the email body\n",
        "  body = ' '.join(m for m in flatten_to_string(msg.get_payload())\n",
        "  if type(m) == str)\n",
        "\n",
        "  if not body:\n",
        "    body = \"\"\n",
        "\n",
        "  return subject + ' ' + body"
      ],
      "metadata": {
        "id": "cOWx6Nnk3NOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `extract_email_text` function extracts the subject and body text from an email file specified by its path. It first loads the file using `email.message_from_file` function from the `email` library. The `Subject` field of the message is extracted and assigned to the subject variable. The function then calls the `flatten_to_string` function to extract the payload (text) of the message and concatenate it into a single string. The resulting string is assigned to the `body` variable. If either the `subject` or `body` variables are empty, they are assigned empty strings. Finally, the `subject` and `body` strings are concatenated with a space character and returned."
      ],
      "metadata": {
        "id": "ItilR2hq28tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" \".join([\"Hello Mr X, \", \"How are you?\"]))"
      ],
      "metadata": {
        "id": "nlK9UKfk7HVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the different parts of the email into a flat list of strings\n",
        "def flatten_to_string(parts):\n",
        "  ret = []\n",
        "  if type(parts) == str:\n",
        "    ret.append(parts)\n",
        "  elif type(parts) == list:\n",
        "    for part in parts:\n",
        "      ret += flatten_to_string(part)\n",
        "  elif parts.get_content_type == 'text/plain':\n",
        "    ret += parts.get_payload()\n",
        "  return ret"
      ],
      "metadata": {
        "id": "1SflIvA6umVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `flatten_to_string` function takes a nested list of strings and flattens it into a flat list of strings. If the input `parts` is a string, it is directly appended to the `ret` list. If the input `parts` is a list, the function recursively calls itself on each element of the list and concatenates the resulting lists. If the input parts is an email message object and has a content type of `text/plain`, the payload (i.e., the actual text) is appended to the ret list.\n",
        "\n",
        "In the context of the email library in Python, `get_payload` is a method of the `email.message.Message` class that returns the payload (i.e., the content) of an email message object. The payload can be any object, but it is typically a string that represents the body of the email message."
      ],
      "metadata": {
        "id": "D3OfLdOc3KMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {}\n",
        "\n",
        "# Read the labels\n",
        "with open(LABELS_FILE) as f:\n",
        "  for line in f:\n",
        "    line = line.strip()\n",
        "    label, key = line.split()\n",
        "    if label.lower() == 'ham':\n",
        "      labels[key.split('/')[-1]] = 1\n",
        "    else:\n",
        "      labels[key.split('/')[-1]] = 0"
      ],
      "metadata": {
        "id": "HKPTUDhCpyxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code is reading a set of labels from a file and storing them in a Python dictionary called labels. The keys of the dictionary are file names, and the values are binary labels indicating whether each file is classified as \"ham\" or \"spam\".\n",
        "\n",
        "* `labels = {}`: This initializes an empty dictionary called labels that will be used to store the file labels.\n",
        "* `with open(LABELS_FILE) as f`: This opens a file specified by the LABELS_FILE variable and creates a file object f to read its contents. The with statement is used to ensure that the file is properly closed after it is read.\n",
        "* `for line in f`: This iterates over each line in the file, where line is a string containing the text on that line.\n",
        "* `line = line.strip()`: This removes any leading or trailing whitespace from the line.\n",
        "* `label, key = line.split()`: This splits the line into two parts, separated by whitespace. The first part is assigned to the label variable, and the second part is assigned to the key variable.\n",
        "* `key.split('/')[-1]` extracts the filename from the key by splitting it on the forward slash (/) character and taking the last element of the resulting list (i.e., the filename without any directories). It then checks if the label is \"ham\" (case-insensitive) and assigns a value of 1 if it is, or 0 if it is not. The resulting value is stored in the labels dictionary under the filename key."
      ],
      "metadata": {
        "id": "-9RpItW25Ki9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(key)\n",
        "print(key.split('/'))\n",
        "print(key.split('/')[-1])"
      ],
      "metadata": {
        "id": "DJ54GTYY6cu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just to show the some elements from labels variable\n",
        "print(list(labels.items())[:5])"
      ],
      "metadata": {
        "id": "sQlfMr9G9FmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(labels))"
      ],
      "metadata": {
        "id": "amQsn5QD0pQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_email_files():\n",
        "  X = []\n",
        "  y = []\n",
        "  for filename in labels:\n",
        "    email_str = extract_email_text(os.path.join(DATA_DIR, filename))\n",
        "    X.append(email_str)\n",
        "    y.append(labels[filename])\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "sDx1ox60dmaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code defines a function called read_email_files() that reads a set of email files, extracts their contents, and returns the contents and corresponding labels.\n",
        "\n",
        "* `X = []`: This initializes an empty list called X that will be used to store the email contents.\n",
        "* `y = []`: This initializes an empty list called y that will be used to store the corresponding labels.\n",
        "* `for i in labels`: This iterates over the keys (i.e., filenames) of labels dictionary.\n",
        "* `email_str = extract_email_text(os.path.join(DATA_DIR, filename))`: This extracts the contents of the email file with the given filename by calling the extract_email_text() function, passing in the path to the email file (constructed by joining the DATA_DIR path and the filename).\n",
        "* `X.append(email_str)`: This adds the email contents to the X list.\n",
        "* `y.append(labels[filename])`: This adds the corresponding label to the y list by looking up the label in the labels dictionary using the current filename as the key.\n",
        "* `return X, y`: This returns the X and y lists as a tuple."
      ],
      "metadata": {
        "id": "837Sezaf6Osl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = read_email_files()\n",
        "print(f\"Email: {X[100]}\\nLabel: {y[100]}\")"
      ],
      "metadata": {
        "id": "v0JADxtEIvSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    train_size=0.7,\n",
        "    random_state=2\n",
        "  )"
      ],
      "metadata": {
        "id": "JLupjvcipFJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have prepared the raw data, you need to do some further processing of the tokens to convert each email to a vector representation that a machine learning algorithm accepts as input."
      ],
      "metadata": {
        "id": "VfOGY_dGxCeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vector = vectorizer.fit_transform(X_train)\n",
        "X_test_vector = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "raX3xlQ7pSqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code is using the scikit-learn library's CountVectorizer class to convert the raw text data of email messages into a numerical representation that can be used as input to machine learning algorithms.\n",
        "\n",
        "* `vectorizer = CountVectorizer()`: This creates an instance of the CountVectorizer class, which is used to convert text data into a bag-of-words representation, where each word in the text is treated as a separate feature and the frequency of occurrence of each word is counted.\n",
        "* `X_train_vector = vectorizer.fit_transform(X_train)`: This applies the fit_transform() method of the CountVectorizer object to the training data X_train, which learns the vocabulary of the training data (i.e., the set of unique words that appear in the emails) and returns a sparse matrix X_train_vector where each row corresponds to an email and each column corresponds to a word in the vocabulary, with the values indicating the frequency of occurrence of the corresponding word in the corresponding email. The fit_transform() method is a combination of two steps: first, it fits the vectorizer to the training data (i.e., it learns the vocabulary of the training data), and second, it transforms the training data into a bag-of-words representation using the learned vocabulary.\n",
        "* `X_test_vector = vectorizer.transform(X_test)`: This applies the transform() method of the CountVectorizer object to the test data X_test, which uses the vocabulary learned from the training data to convert the test data into a bag-of-words representation, returning a sparse matrix X_test_vector. The transform() method only transforms the test data into a bag-of-words representation using the vocabulary learned from the training data. It does not update the vocabulary or learn new features."
      ],
      "metadata": {
        "id": "9Jk03BVQ7XMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifier -- Naive Bayes"
      ],
      "metadata": {
        "id": "KT-LH6tQzCik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is a probabilistic machine learning algorithm based on Bayes' theorem, which describes the probability of a hypothesis given evidence. In the context of classification, the hypothesis corresponds to the class label and the evidence corresponds to the input features.\n",
        "\n",
        "Multinomial Naive Bayes (`MultinomialNB`): Assumes that the input features are discrete counts, such as word frequencies in text classification."
      ],
      "metadata": {
        "id": "QUiKj2Xa2l92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Initialize the classifier and make label predictions\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_vector, y_train)\n",
        "y_pred = mnb.predict(X_test_vector)\n",
        "\n",
        "# Evaluation metrics\n",
        "print('Accuracy: {:.3f}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('F1 score: {:.3f}'.format(f1_score(y_test, y_pred)))\n",
        "print('ROC AUC: {:.3f}'.format(roc_auc_score(y_test, y_pred)))"
      ],
      "metadata": {
        "id": "zeOFaiQUxgV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction on a real sample"
      ],
      "metadata": {
        "id": "5n7QxmcNZV_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/AICS/sample_email.txt', 'r') as f:\n",
        "  email_str = f.read()\n",
        "  print(f\"Email: \\n{email_str}\")\n",
        "\n",
        "email_vector = vectorizer.transform([email_str])\n",
        "prediction = mnb.predict(email_vector)\n",
        "\n",
        "if prediction == 1:\n",
        "  print('\\nPrediction: Spam')\n",
        "else:\n",
        "  print('\\nPrediction: Ham')\n"
      ],
      "metadata": {
        "id": "44AjHZZRTCZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice task\n",
        "Try different ML algorithms for spam vs ham classification and improve the classification accuracy. In addition to classification accuracy, use confusion matrix to report performance."
      ],
      "metadata": {
        "id": "L9BIfwPu66UN"
      }
    }
  ]
}
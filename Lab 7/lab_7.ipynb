{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1E62EZC7bhFe93yHTHvzo_3uCvzZKC8GO",
      "authorship_tag": "ABX9TyMf6dLCSDOS7Tzrxx3IjdUE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasan-rakibul/AI-cybersec/blob/main/Lab%207/lab_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spam fighting using machine learning"
      ],
      "metadata": {
        "id": "s5PijsUCmpn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reference book:** Chio, C., & Freeman, D. (2018). *Machine learning and security: Protecting systems with data and algorithms* (First edition). Oâ€™Reilly Media. (page 18)\n",
        "\n",
        "**Reference programs:** https://github.com/oreilly-mlsec/book-resources/tree/master/chapter1"
      ],
      "metadata": {
        "id": "R0nQ3bNKmn-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Access dataset"
      ],
      "metadata": {
        "id": "37dMGHl0WSuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the 2007 TREC Public Spam Corpus dataset that to explore spam versus ham (not spam) classification problem. This is a lightly cleaned raw email message corpus containing 75,419 messages collected from an email server over a three-month period in 2007. One-third of the dataset is made up of spam examples, and the rest is ham."
      ],
      "metadata": {
        "id": "ExCepMxvyYMF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTQvuQkPhk4_"
      },
      "source": [
        "### Downloading 2007 TREC Public Spam Corpus\n",
        "1. Read and accept the \"Agreement for use\"\n",
        "   [https://plg.uwaterloo.ca/~gvcormac/treccorpus07/](https://plg.uwaterloo.ca/~gvcormac/treccorpus07/)\n",
        "2. You will find \"255 MB Corpus (trec07p.tgz)\". Right click on that link and \"save as\" the dataset. You can keep the data on your Google drive folder (e.g., Colab\n",
        "Notebooks/AICS/ folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "eMqxdtn0JJeP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, mount Google Drive\n",
        "if not os.path.exists(\"trec07p\"):\n",
        "  !tar xzf \"/content/drive/MyDrive/Colab Notebooks/AICS/trec07p.tgz\""
      ],
      "metadata": {
        "id": "YfUQcVuavGkG"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above command can be read as \"extract the contents of the `trec07p.tgz` archive file located in the directory `/content/drive/MyDrive/Colab Notebooks/AICS`.\n",
        "\n",
        "* tar is a command-line utility in Unix-based operating systems used for working with TAR archives.\n",
        "* x is a command-line option used to extract the contents of the archive.\n",
        "* z is a command-line option used to decompress the archive using the gzip utility before extracting its contents.\n",
        "* f is a command-line option used to specify the name of the archive file to work with."
      ],
      "metadata": {
        "id": "dRFkNbAn4pE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = './trec07p/data/'\n",
        "LABELS_FILE = './trec07p/full/index'\n",
        "TRAINING_SET_RATIO = 0.7"
      ],
      "metadata": {
        "id": "F8GSD2MyU1pU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Email processing"
      ],
      "metadata": {
        "id": "CWF_pPccyzit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first define a set of functions to help with loading and preprocessing the data and labels, as demonstrated in the following code."
      ],
      "metadata": {
        "id": "KBi1GgfnsjYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import email"
      ],
      "metadata": {
        "id": "MvtkPpuyqQy_"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract subject and body text from a signle email file\n",
        "def extract_email_text(path):\n",
        "  # Load a single email file from an input file\n",
        "  with open(path, errors='ignore') as f:\n",
        "    msg = email.message_from_file(f)\n",
        "  if not msg:\n",
        "    return \"\"\n",
        "\n",
        "  # Read the email subject\n",
        "  subject = msg['Subject']\n",
        "  if not subject:\n",
        "    subject = ''\n",
        "\n",
        "  # Read the email body\n",
        "  body = ' '.join(m for m in flatten_to_string(msg.get_payload())\n",
        "  if type(m) == str)\n",
        "\n",
        "  if not body:\n",
        "    body = \"\"\n",
        "\n",
        "  return subject + ' ' + body"
      ],
      "metadata": {
        "id": "cOWx6Nnk3NOl"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `extract_email_text` function extracts the subject and body text from an email file specified by its path. It first loads the file using `email.message_from_file` function from the `email` library. The `Subject` field of the message is extracted and assigned to the subject variable. The function then calls the `flatten_to_string` function to extract the payload (text) of the message and concatenate it into a single string. The resulting string is assigned to the `body` variable. If either the `subject` or `body` variables are empty, they are assigned empty strings. Finally, the `subject` and `body` strings are concatenated with a space character and returned."
      ],
      "metadata": {
        "id": "ItilR2hq28tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the different parts of the email into a flat list of strings\n",
        "def flatten_to_string(parts):\n",
        "  ret = []\n",
        "  if type(parts) == str:\n",
        "    ret.append(parts)\n",
        "  elif type(parts) == list:\n",
        "    for part in parts:\n",
        "      ret += flatten_to_string(part)\n",
        "  elif parts.get_content_type == 'text/plain':\n",
        "    ret += parts.get_payload()\n",
        "  return ret"
      ],
      "metadata": {
        "id": "1SflIvA6umVx"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `flatten_to_string` function takes a nested list of strings and flattens it into a flat list of strings. If the input `parts` is a string, it is directly appended to the `ret` list. If the input `parts` is a list, the function recursively calls itself on each element of the list and concatenates the resulting lists. If the input parts is an email message object and has a content type of `text/plain`, the payload (i.e., the actual text) is appended to the ret list.\n",
        "\n",
        "In the context of the email library in Python, `get_payload` is a method of the `email.message.Message` class that returns the payload (i.e., the content) of an email message object. The payload can be any object, but it is typically a string that represents the body of the email message."
      ],
      "metadata": {
        "id": "D3OfLdOc3KMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {}\n",
        "\n",
        "# Read the labels\n",
        "with open(LABELS_FILE) as f:\n",
        "  for line in f:\n",
        "    line = line.strip()\n",
        "    label, key = line.split()\n",
        "    if label.lower() == 'ham':\n",
        "      labels[key.split('/')[-1]] = 1\n",
        "    else:\n",
        "      labels[key.split('/')[-1]] = 0"
      ],
      "metadata": {
        "id": "HKPTUDhCpyxU"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code is reading a set of labels from a file and storing them in a Python dictionary called labels. The keys of the dictionary are file names, and the values are binary labels indicating whether each file is classified as \"ham\" or \"spam\".\n",
        "\n",
        "* `labels = {}`: This initializes an empty dictionary called labels that will be used to store the file labels.\n",
        "* `with open(LABELS_FILE) as f`: This opens a file specified by the LABELS_FILE variable and creates a file object f to read its contents. The with statement is used to ensure that the file is properly closed after it is read.\n",
        "* `for line in f`: This iterates over each line in the file, where line is a string containing the text on that line.\n",
        "* `line = line.strip()`: This removes any leading or trailing whitespace from the line.\n",
        "* `label, key = line.split()`: This splits the line into two parts, separated by whitespace. The first part is assigned to the label variable, and the second part is assigned to the key variable.\n",
        "* `key.split('/')[-1]` extracts the filename from the key by splitting it on the forward slash (/) character and taking the last element of the resulting list (i.e., the filename without any directories). It then checks if the label is \"ham\" (case-insensitive) and assigns a value of 1 if it is, or 0 if it is not. The resulting value is stored in the labels dictionary under the filename key."
      ],
      "metadata": {
        "id": "-9RpItW25Ki9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(key)\n",
        "print(key.split('/'))\n",
        "print(key.split('/')[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ54GTYY6cu_",
        "outputId": "488d9b9b-5e5c-4e48-f5a7-569ef6878f50"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../data/inmail.75419\n",
            "['..', 'data', 'inmail.75419']\n",
            "inmail.75419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amQsn5QD0pQP",
        "outputId": "17074e22-0ddf-4879-bafd-fd623583e5e3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75419"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_email_files():\n",
        "  X = []\n",
        "  y = []\n",
        "  for filename in labels:\n",
        "    email_str = extract_email_text(os.path.join(DATA_DIR, filename))\n",
        "    X.append(email_str)\n",
        "    y.append(labels[filename])\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "sDx1ox60dmaW"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code defines a function called read_email_files() that reads a set of email files, extracts their contents, and returns the contents and corresponding labels.\n",
        "\n",
        "* `X = []`: This initializes an empty list called X that will be used to store the email contents.\n",
        "* `y = []`: This initializes an empty list called y that will be used to store the corresponding labels.\n",
        "* `for i in labels`: This iterates over the keys (i.e., filenames) of labels dictionary.\n",
        "* `email_str = extract_email_text(os.path.join(DATA_DIR, filename))`: This extracts the contents of the email file with the given filename by calling the extract_email_text() function, passing in the path to the email file (constructed by joining the DATA_DIR path and the filename).\n",
        "* `X.append(email_str)`: This adds the email contents to the X list.\n",
        "* `y.append(labels[filename])`: This adds the corresponding label to the y list by looking up the label in the labels dictionary using the current filename as the key.\n",
        "* `return X, y`: This returns the X and y lists as a tuple."
      ],
      "metadata": {
        "id": "837Sezaf6Osl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = read_email_files()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    train_size=TRAINING_SET_RATIO,\n",
        "    random_state=2\n",
        "  )"
      ],
      "metadata": {
        "id": "JLupjvcipFJJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Email: {X[100]}\\nLabel: {y[100]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBLmrfXhikdp",
        "outputId": "5630b4d9-ded0-4a8d-be16-357ba0f4491a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email: How r u lately \n",
            "Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have prepared the raw data, you need to do some further processing of the tokens to convert each email to a vector representation that a machine learning algorithm accepts as input."
      ],
      "metadata": {
        "id": "VfOGY_dGxCeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vector = vectorizer.fit_transform(X_train)\n",
        "X_test_vector = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "raX3xlQ7pSqW"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code is using the scikit-learn library's CountVectorizer class to convert the raw text data of email messages into a numerical representation that can be used as input to machine learning algorithms.\n",
        "\n",
        "* `vectorizer = CountVectorizer()`: This creates an instance of the CountVectorizer class, which is used to convert text data into a bag-of-words representation, where each word in the text is treated as a separate feature and the frequency of occurrence of each word is counted.\n",
        "* `X_train_vector = vectorizer.fit_transform(X_train)`: This applies the fit_transform() method of the CountVectorizer object to the training data X_train, which learns the vocabulary of the training data (i.e., the set of unique words that appear in the emails) and returns a sparse matrix X_train_vector where each row corresponds to an email and each column corresponds to a word in the vocabulary, with the values indicating the frequency of occurrence of the corresponding word in the corresponding email. The fit_transform() method is a combination of two steps: first, it fits the vectorizer to the training data (i.e., it learns the vocabulary of the training data), and second, it transforms the training data into a bag-of-words representation using the learned vocabulary.\n",
        "* `X_test_vector = vectorizer.transform(X_test)`: This applies the transform() method of the CountVectorizer object to the test data X_test, which uses the vocabulary learned from the training data to convert the test data into a bag-of-words representation, returning a sparse matrix X_test_vector. The transform() method only transforms the test data into a bag-of-words representation using the vocabulary learned from the training data. It does not update the vocabulary or learn new features."
      ],
      "metadata": {
        "id": "9Jk03BVQ7XMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifier -- Naive Bayes"
      ],
      "metadata": {
        "id": "KT-LH6tQzCik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is a probabilistic machine learning algorithm based on Bayes' theorem, which describes the probability of a hypothesis given evidence. In the context of classification, the hypothesis corresponds to the class label and the evidence corresponds to the input features.\n",
        "\n",
        "Multinomial Naive Bayes (`MultinomialNB`): Assumes that the input features are discrete counts, such as word frequencies in text classification."
      ],
      "metadata": {
        "id": "QUiKj2Xa2l92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Initialize the classifier and make label predictions\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_vector, y_train)\n",
        "y_pred = mnb.predict(X_test_vector)\n",
        "\n",
        "# Evaluation metrics\n",
        "print('Accuracy: {:.3f}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('F1 score: {:.3f}'.format(f1_score(y_test, y_pred)))\n",
        "print('ROC AUC: {:.3f}'.format(roc_auc_score(y_test, y_pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeOFaiQUxgV3",
        "outputId": "02f69b3a-2f38-4482-8c22-8a9f2190abd6"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956\n",
            "F1 score: 0.937\n",
            "ROC AUC: 0.962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction on a real sample"
      ],
      "metadata": {
        "id": "5n7QxmcNZV_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/ICT607/sample_email.txt', 'r') as f:\n",
        "  email_str = f.read()\n",
        "\n",
        "email_vector = vectorizer.transform([email_str])\n",
        "prediction = mnb.predict(email_vector)\n",
        "\n",
        "if prediction == 1:\n",
        "  print('The email is predicted to be ham.')\n",
        "else:\n",
        "  print('The email is predicted to be spam.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44AjHZZRTCZ4",
        "outputId": "24db5178-01ad-4392-da17-900fc35b8f57"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The email is predicted to be spam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice task\n",
        "Try different ML algorithms for spam vs ham classification and improve the classification accuracy. In addition to classification accuracy, use confusion matrix to report performance."
      ],
      "metadata": {
        "id": "L9BIfwPu66UN"
      }
    }
  ]
}